Yep ‚Äî this is one of those questions where the database behaves very predictably, but it‚Äôs still good you‚Äôre double-checking üëç
I‚Äôll answer it cleanly and practically.

‚∏ª

1Ô∏è‚É£ What exactly happens when you DROP TABLE?

When you run:

DROP TABLE my_table;

üëâ Everything tied to that table is dropped automatically, including:
	‚Ä¢	‚úÖ The table data
	‚Ä¢	‚úÖ All indexes (primary, unique, secondary)
	‚Ä¢	‚úÖ Constraints
	‚Ä¢	Primary key
	‚Ä¢	Unique key
	‚Ä¢	Check constraints
	‚Ä¢	‚úÖ Triggers defined on that table
	‚Ä¢	‚úÖ Table-level privileges
	‚Ä¢	‚úÖ Auto-increment state
	‚Ä¢	‚úÖ Storage metadata (tablespace segments)

In short:
After DROP TABLE, the database forgets the table ever existed.

There is no leftover index, constraint, or metadata hanging around.

‚∏ª

2Ô∏è‚É£ Can I recreate the table using the same old DDL?

‚úî Yes ‚Äî 100% safe.

Example:

DROP TABLE my_table;

CREATE TABLE my_table (
  id INT PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  INDEX idx_email (email)
);

This works exactly the same as the first time you created it.

Why?
	‚Ä¢	Index names are scoped to the table
	‚Ä¢	Since the table is gone, the old index names are gone too
	‚Ä¢	The database treats this as a brand-new object

‚∏ª

3Ô∏è‚É£ Do I need to change index names?

‚ùå No, not if the table was dropped

You can safely reuse:
	‚Ä¢	Same table name
	‚Ä¢	Same index names
	‚Ä¢	Same constraint names

Example (perfectly fine):

CREATE TABLE orders (
  order_id BIGINT PRIMARY KEY,
  customer_id BIGINT,
  INDEX idx_customer_id (customer_id)
);

Even if idx_customer_id existed earlier ‚Äî it was destroyed with the table.

‚∏ª

4Ô∏è‚É£ When would index name conflicts happen?

Only in these cases üëá

‚ùå Case 1: Table NOT dropped

CREATE INDEX idx_email ON my_table(email);
-- ERROR if idx_email already exists on my_table

‚ùå Case 2: Some databases + global constraint naming

(rare, DB-specific, mostly with constraints ‚Äî not indexes)

Example:

CONSTRAINT pk_orders PRIMARY KEY (id)

If the table still exists, constraint names must be unique within schema rules.

But again ‚Äî not applicable after DROP TABLE.

‚∏ª

5Ô∏è‚É£ Special notes (important in real systems)

üîπ Foreign keys from other tables

If another table references this table:

orders.customer_id ‚Üí customers.id

Then:
	‚Ä¢	DROP TABLE customers may ‚ùå fail
	‚Ä¢	Or require:

DROP TABLE customers CASCADE;

Be careful in PROD.

‚∏ª

üîπ Migration tools (Flyway / Liquibase)

Since you mentioned Flyway earlier in other chats:
	‚Ä¢	Reusing same DDL after DROP TABLE is fine
	‚Ä¢	But never rely on DROP+CREATE in PROD migrations unless intentional
	‚Ä¢	Prefer:
	‚Ä¢	ALTER TABLE
	‚Ä¢	versioned migrations

‚∏ª

6Ô∏è‚É£ Quick decision guide

Scenario	Safe to reuse same DDL?
DROP TABLE + CREATE TABLE	‚úÖ Yes
Same index names	‚úÖ Yes
Same constraint names	‚úÖ Yes
Table not dropped	‚ùå No
FK dependencies exist	‚ö†Ô∏è Check first


‚∏ª

‚úÖ Bottom line
	‚Ä¢	DROP TABLE removes everything ‚Äî data, indexes, constraints
	‚Ä¢	Reusing the same old DDL is totally safe
	‚Ä¢	No need to rename indexes
	‚Ä¢	Just watch out for foreign key dependencies in other tables

If you want, tell me:
	‚Ä¢	DB type (MySQL / MariaDB / Oracle / Postgres)
	‚Ä¢	PROD vs DEV
	‚Ä¢	Migration tool (Flyway?)

I‚Äôll give you a safe, production-grade checklist for that environment.



It depends what you‚Äôre trying to achieve: ‚Äúempty the data‚Äù or ‚Äúrebuild the object.‚Äù In most real systems, TRUNCATE is the best way to empty a table; DROP+CREATE is only better when you truly want to recreate/reshape the table.

Here‚Äôs the practical comparison (works for MySQL/MariaDB and broadly similar elsewhere):

‚∏ª

What each one does

TRUNCATE TABLE t;
	‚Ä¢	Empties all rows fast (typically deallocates/recreates the data pages)
	‚Ä¢	Keeps the table definition intact
	‚Ä¢	Keeps indexes (definition remains; contents become empty)
	‚Ä¢	Usually resets AUTO_INCREMENT back to start (common in MySQL/MariaDB)
	‚Ä¢	Permissions remain (since the table object remains)

DROP TABLE t; + CREATE TABLE t (...);
	‚Ä¢	Deletes the table object completely
	‚Ä¢	Drops everything tied to the table:
	‚Ä¢	indexes, constraints, triggers, permissions, auto-inc state, storage metadata
	‚Ä¢	Recreates it from scratch using your DDL

‚∏ª

Which is ‚Äúbetter‚Äù in practice?

‚úÖ Use TRUNCATE when:

You want to quickly clear data but keep structure the same.

Best for:
	‚Ä¢	staging / temp tables
	‚Ä¢	reloading reference tables
	‚Ä¢	clearing test data
	‚Ä¢	bulk reload pipelines

Why it‚Äôs better:
	‚Ä¢	fast
	‚Ä¢	simple
	‚Ä¢	less risk (you don‚Äôt accidentally recreate with wrong charset/engine/partitioning/etc.)
	‚Ä¢	keeps grants/permissions and object identity

‚∏ª

‚úÖ Use DROP + CREATE when:

You want to rebuild the table definition or cleanly reset everything.

Best for:
	‚Ä¢	changing table structure drastically (engine/partitioning/row format)
	‚Ä¢	removing/replacing triggers
	‚Ä¢	resetting privileges intentionally
	‚Ä¢	clearing ‚Äúobject baggage‚Äù in dev/test environments

Why it can be better:
	‚Ä¢	ensures you recreate exactly as per DDL
	‚Ä¢	wipes out triggers/constraints you may not want

‚∏ª

Key differences that matter (MySQL/MariaDB reality)

1) Foreign Keys
	‚Ä¢	TRUNCATE often fails if the table is referenced by a foreign key (even if child table is empty), unless you drop/disable constraints or handle dependencies properly.
	‚Ä¢	DROP also fails unless you drop dependent tables first, or you use CASCADE in databases that support it (MySQL doesn‚Äôt do DROP TABLE ... CASCADE like Postgres).

So if there are FK relationships, you must plan the order either way.

2) Triggers
	‚Ä¢	TRUNCATE does not fire row triggers (DELETE triggers).
	‚Ä¢	DROP+CREATE removes triggers entirely (unless recreated).

3) Logging / Replication
	‚Ä¢	Both are DDL-ish operations and can behave differently from DELETE:
	‚Ä¢	TRUNCATE is typically ‚Äúall-or-nothing‚Äù and efficient
	‚Ä¢	replication/binlog semantics differ by setup, but both generally replicate fine

4) Permissions & Object Identity
	‚Ä¢	TRUNCATE keeps grants.
	‚Ä¢	DROP+CREATE loses grants unless your DDL re-adds them (or your DB has default privilege automation).

This alone is a big reason teams prefer TRUNCATE for ‚Äúemptying‚Äù.

5) Safer for production
	‚Ä¢	TRUNCATE is safer than DROP+CREATE because you‚Äôre not rebuilding the object (less chance of recreating with slightly different DDL).
	‚Ä¢	DROP+CREATE is riskier: wrong DDL, missing indexes, wrong charset/collation, wrong engine, missing grants.

‚∏ª

Recommendation (simple rule)
	‚Ä¢	If your goal is just to clear all rows: ‚úÖ TRUNCATE
	‚Ä¢	If your goal is to rebuild schema / remove triggers / reset everything: ‚úÖ DROP + CREATE
	‚Ä¢	If you need trigger behavior or conditional delete: use DELETE (slower, but controlled)

‚∏ª

If you tell me:
	1.	Is it MariaDB?
	2.	Does the table have foreign keys (parent/child)?
	3.	Is this DEV/UAT/PROD?

‚Ä¶I‚Äôll suggest the safest exact approach + the right SQL sequence for your case.



For 5 million rows, the fastest and safest ‚Äúempty the table‚Äù option is usually TRUNCATE ‚Äî if you‚Äôre allowed to use it (no FK blockers) and you don‚Äôt need row-level deletes.

Speed (typical)
	1.	TRUNCATE ‚úÖ fastest

	‚Ä¢	It doesn‚Äôt delete row-by-row; it effectively resets the table‚Äôs storage.
	‚Ä¢	For 5M rows it‚Äôs usually seconds (depends on disk/engine).

	2.	DROP + CREATE ‚úÖ also very fast (often comparable to TRUNCATE)

	‚Ä¢	Drops the object and recreates it.
	‚Ä¢	Time mostly depends on: table metadata + index creation (but empty table index creation is quick).

	3.	DELETE FROM table; ‚ùå slowest

	‚Ä¢	Deletes rows one by one (even if optimized), generates lots of undo/log activity.
	‚Ä¢	For 5M rows it can be minutes to hours depending on system and indexes.

‚∏ª

Which one is the ‚Äúgood option‚Äù?

‚úÖ If your goal is ‚Äúempty the table and reload‚Äù

Use TRUNCATE TABLE if:
	‚Ä¢	The table is not referenced by foreign keys (or you‚Äôre clearing children first)
	‚Ä¢	You don‚Äôt need triggers/audit per row
	‚Ä¢	You‚Äôre okay with AUTO_INCREMENT resetting (common in MariaDB/MySQL)

Why it‚Äôs best: fastest, keeps grants, keeps schema exactly the same.

‚∏ª

‚úÖ Use DROP + CREATE if you want a clean rebuild

Pick this if:
	‚Ä¢	You want to recreate indexes/constraints fresh
	‚Ä¢	You want to remove triggers / reset permissions intentionally
	‚Ä¢	You‚Äôre doing a controlled environment reset (DEV/UAT)

Caution: In PROD, this is riskier because if the DDL is slightly different or you forget grants, you can break apps.

‚∏ª

The big ‚Äúblocker‚Äù: Foreign keys

If other tables reference it:
	‚Ä¢	TRUNCATE may fail with FK restrictions.
	‚Ä¢	DROP may also fail unless you drop dependent tables first.

So the best approach often becomes:
‚úÖ TRUNCATE child tables first, then parent.

‚∏ª

Practical recommendation for 5M rows (MariaDB/MySQL)

If no foreign keys:

‚úÖ TRUNCATE is the best option.

TRUNCATE TABLE my_table;

If foreign keys exist:
	‚Ä¢	Truncate in correct order: children ‚Üí parent
	‚Ä¢	Or use a controlled approach (sometimes temporarily disable FK checks in non-prod; avoid in prod unless you know the risks).

‚∏ª

Final answer in one line

For 5 million rows:
	‚Ä¢	Fastest: TRUNCATE (or DROP+CREATE, both very fast)
	‚Ä¢	Best option generally: TRUNCATE, because it‚Äôs fast and keeps the table object/permissions stable
	‚Ä¢	Avoid: DELETE unless you must delete selectively or need triggers/audit

If you tell me whether this table is referenced by foreign keys and whether this is PROD, I‚Äôll tell you the exact safest sequence (including child/parent order) without guesswork.



Your approach is broadly right (bulk-load into a staging table, then move into final), and for millions of rows the biggest performance win is exactly what you‚Äôre thinking: load first, add indexes/PK later.

Below is a reviewed + slightly improved version that avoids a few common traps.

‚∏ª

Review of your 8-step plan

‚úÖ Steps 1‚Äì5 are good
	1.	Export from DB2 ‚úÖ
	2.	Create staging table in MariaDB without PK/index ‚úÖ
	3.	Load data ‚úÖ
	4.	Create PK ‚úÖ
	5.	Create secondary indexes ‚úÖ

This is the standard bulk-load optimization pattern.

‚ö†Ô∏è Step 6 (delete soft-deleted rows in staging)

This is OK, but often you can do better:
	‚Ä¢	If DB2 export can filter soft_delete=false, do it there (best).
	‚Ä¢	If not, do it after load as you planned.

But: deleting millions of rows can be expensive. A faster alternative is to insert only non-deleted into final (skip deletes entirely).

‚ö†Ô∏è Steps 7‚Äì8 (export staging then import to actual)

You usually don‚Äôt need to export/import again inside MariaDB.

Instead of:
	‚Ä¢	Export staging ‚Üí import into final

Prefer:
	‚Ä¢	INSERT INTO final SELECT ... FROM staging ... (fast, in-server, no file I/O)
	‚Ä¢	or RENAME TABLE strategy when possible.

‚∏ª

Recommended refined approach (faster + simpler)

Option A (Most common + efficient)
	1.	Export from DB2 (DEL/CSV)
	2.	Create staging table (no PK, no secondary indexes)
	3.	Bulk load into staging (LOAD DATA INFILE / your import tool)
	4.	Data cleanup/transforms in staging (split pipe columns, trim, null handling)
	5.	Insert into final:

INSERT INTO final_table (col1, col2, account_number, account_indicator, ...)
SELECT col1, col2,
       CASE WHEN LOCATE('|', src_col)>0 THEN SUBSTRING_INDEX(src_col,'|',1) ELSE src_col END,
       CASE WHEN LOCATE('|', src_col)>0 THEN SUBSTRING_INDEX(src_col,'|',-1) ELSE NULL END,
       ...
FROM staging
WHERE soft_delete <> 1;   -- or soft_delete = 0

	6.	Build indexes on final (if final is empty, build them after the insert; if final already has data, plan carefully)
	7.	Verify (counts/hashes/spot checks)
	8.	Drop staging

‚úÖ No second export/import needed.

‚∏ª

Your specific questions

1) Primary key: during table creation or after data load?

After data loading is usually faster for big loads.

Why:
	‚Ä¢	With PK present, every insert updates the PK index (random I/O and overhead)
	‚Ä¢	Creating PK at the end is a single bulk build (often faster)

Exception:
If you need PK to:
	‚Ä¢	enforce uniqueness early, or
	‚Ä¢	de-duplicate during load
then create it early (but performance will drop).

Best practice for staging: load first, validate uniqueness, then add PK (if needed).

‚∏ª

2) Index creation: after loading or before?

After loading (almost always).

Same reason: maintaining indexes row-by-row during insert is slow.

So:
	‚Ä¢	staging: no indexes before load
	‚Ä¢	final: if final is empty ‚Üí load then create indexes
	‚Ä¢	final: if final already has data ‚Üí depends, but still often load then index (or do online schema changes carefully)

‚∏ª

3) Do I need a primary key for staging table?

Usually NO.

Staging is for:
	‚Ä¢	raw landing
	‚Ä¢	transform
	‚Ä¢	verification

So it can be a heap-like table (no PK) for speed.

When staging PK is useful:
	‚Ä¢	You need to deduplicate (e.g., PK from source is supposed to be unique and you want the DB to enforce it)
	‚Ä¢	You will join staging to other staging tables repeatedly and need performance
	‚Ä¢	You need to track bad rows uniquely (add a surrogate row_id)

A good compromise is:
	‚Ä¢	Add a simple surrogate key only if needed:

row_id BIGINT AUTO_INCREMENT PRIMARY KEY

This doesn‚Äôt enforce business uniqueness but helps troubleshooting.

‚∏ª

Important improvements (based on your earlier ‚Äúpipe split‚Äù case)

Since you have DB2 field like AccountNumber|AccountIndicator:

Do the split in MariaDB staging ‚Üí final insert (fast, controlled).
Don‚Äôt try to pre-split in the file unless necessary.

Also, instead of ‚Äúdelete soft_delete rows in staging‚Äù, you can just:
	‚Ä¢	keep staging as-is
	‚Ä¢	insert only non-deleted into final using WHERE soft_delete=0

That avoids a huge delete operation.

‚∏ª

What I would change in your steps (minimal change version)

Your plan, improved:
	1.	Export DB2 ‚úÖ
	2.	Create staging table in MariaDB without PK/index ‚úÖ
	3.	Load to staging ‚úÖ
	4.	Transform in staging (split pipe, normalize) ‚úÖ
	5.	Create final table (proper PK + indexes) ‚úÖ
	6.	Insert into final from staging with WHERE soft_delete=0 ‚úÖ
	7.	Create indexes after insert (if final empty) ‚úÖ
	8.	Verification ‚úÖ
	9.	Drop staging ‚úÖ

This removes the staging export/import loop and reduces total runtime.

‚∏ª

Quick verification checklist (do this for 15M-scale confidence)
	‚Ä¢	Row counts:
	‚Ä¢	DB2 total rows
	‚Ä¢	DB2 active rows (soft_delete=0)
	‚Ä¢	Maria staging total
	‚Ä¢	Maria final total
	‚Ä¢	Null / length validation for split columns
	‚Ä¢	Duplicate check on intended PK columns before creating PK in final
	‚Ä¢	Sampling:
	‚Ä¢	pick 100 random keys from DB2 and verify in Maria
	‚Ä¢	Optional: checksum/hash by key range (if feasible)

‚∏ª

If you tell me:
	1.	Is the final table empty initially or already has data?
	2.	Soft delete column type (0/1? Y/N?)
	3.	How you load (MariaDB LOAD DATA INFILE, mysqlimport, or app script)?

‚Ä¶I‚Äôll give you an exact SQL script template (staging DDL + load command + insert/select split logic + index build order) that matches your setup.


You‚Äôre thinking about the right trade-offs. With 15M+ rows on DB2 PROD and no index on soft_delete, the key is: avoid doing extra work on DB2, and avoid doing extra passes in MariaDB.

1) Will a full export disrupt PROD?

A DB2 export is basically a big read/scan. It won‚Äôt usually ‚Äúblock logon‚Äù, but it can impact:
	‚Ä¢	disk I/O (table scan)
	‚Ä¢	bufferpool churn (pushes other useful pages out)
	‚Ä¢	CPU (formatting/export)
	‚Ä¢	network/disk if writing off-host

Because soft_delete is not indexed, any filter like WHERE soft_delete=0 is still a full scan. So from DB2‚Äôs point of view:
	‚Ä¢	export all rows vs export with WHERE soft_delete=0
‚Üí similar scan cost, but the filtered export writes a smaller file (less I/O after the scan).

Best non-disruptive option (if available):
	‚Ä¢	export from a read replica / HADR standby / restored backup rather than the primary.
If that‚Äôs not available, run on off-peak, and use an isolation that avoids locking (DB2 commonly uses uncommitted read / WITH UR patterns) so you don‚Äôt hold locks while scanning.

‚∏ª

2) Is INSERT INTO final SELECT ... FROM staging slower than exporting staging and importing again?

In most cases: INSERT‚Ä¶SELECT is faster and cleaner than ‚Äúexport staging ‚Üí import final‚Äù, because:

Export+Import path (two extra passes):
	‚Ä¢	reads staging and writes a file (disk heavy)
	‚Ä¢	then reads file again and parses it again into final
	‚Ä¢	more filesystem I/O + CSV parsing twice

INSERT‚Ä¶SELECT path (in-server):
	‚Ä¢	reads staging and writes final directly inside MariaDB
	‚Ä¢	no intermediate file
	‚Ä¢	can apply transforms + filtering in one pass

‚úÖ So yes: INSERT‚Ä¶SELECT is usually the better option.

The only time export/import can win is if your import method is using a highly-optimized bulk loader and your INSERT‚Ä¶SELECT is forced into slow row-by-row work because of indexes/constraints/triggers on the final table.

‚∏ª

3) If INSERT‚Ä¶SELECT is faster, can we do WHERE soft_delete=0? Is it better?

Yes ‚Äî and it‚Äôs usually the best design here.

Instead of:
	‚Ä¢	load all rows into staging
	‚Ä¢	delete soft_delete rows in staging (expensive)
	‚Ä¢	then move everything

Do this:
	‚Ä¢	load all rows into staging
	‚Ä¢	insert only active rows into final:

INSERT INTO final_table (col1, col2, acct_no, acct_ind, ...)
SELECT col1, col2,
       CASE WHEN LOCATE('|', acct_src) > 0 THEN SUBSTRING_INDEX(acct_src,'|',1) ELSE acct_src END AS acct_no,
       CASE WHEN LOCATE('|', acct_src) > 0 THEN SUBSTRING_INDEX(acct_src,'|',-1) ELSE NULL END AS acct_ind,
       ...
FROM staging_table
WHERE soft_delete = 0;

Why it‚Äôs better:
	‚Ä¢	avoids a massive DELETE on staging
	‚Ä¢	writes fewer rows into final
	‚Ä¢	final table stays ‚Äúclean‚Äù from the beginning

‚∏ª

The most important performance rule (MariaDB side)

To make INSERT‚Ä¶SELECT fast for 15M rows:

‚úÖ Final table should have no secondary indexes during load
	‚Ä¢	Create final table with columns, but delay indexes until after data is loaded.
	‚Ä¢	If you need a PK, it‚Äôs still usually faster to add it after the bulk insert (especially if it‚Äôs large and causes random writes).

Recommended order (fastest):
	1.	Create staging (no indexes)
	2.	Load staging (bulk)
	3.	Create final table (structure only; minimal constraints)
	4.	INSERT INTO final SELECT ... WHERE soft_delete=0
	5.	Add PK
	6.	Add secondary indexes (including soft_delete if you still need it in final)
	7.	Verify

If final must enforce uniqueness during insert, then you may need PK earlier ‚Äî but that slows the insert.

‚∏ª

DB2-side filtering: should you filter at export time?

Given no index, filtering doesn‚Äôt save DB2 scan cost, but it can reduce:
	‚Ä¢	export file size
	‚Ä¢	time to transfer
	‚Ä¢	staging load time

So you have two acceptable strategies:

Strategy A (lowest DB2 complexity, more Maria work)
	‚Ä¢	Export all rows
	‚Ä¢	Load staging all rows
	‚Ä¢	Filter during INSERT‚Ä¶SELECT in Maria

Strategy B (smaller file, faster downstream, same DB2 scan)
	‚Ä¢	Export with WHERE soft_delete=0 (still scans)
	‚Ä¢	Load staging fewer rows
	‚Ä¢	Insert into final (less work)

If your bottleneck is file size / load time, pick Strategy B.
If your bottleneck is DB2 impact, both are similar (scan is the expensive part).

‚∏ª

One more high-impact suggestion

If the final table is new and you don‚Äôt need staging after validation, you can even do:
	‚Ä¢	Load into a ‚Äúfinal_new‚Äù table
	‚Ä¢	Build indexes
	‚Ä¢	Swap tables with RENAME TABLE (near-instant cutover)

This avoids long downtime on the real final table.

‚∏ª

Bottom line for your case
	‚Ä¢	Yes, INSERT‚Ä¶SELECT ... WHERE soft_delete=0 is a better option than exporting staging and importing again.
	‚Ä¢	Keep indexes/PK off during load, then create them after.
	‚Ä¢	DB2 filtering won‚Äôt avoid the scan (no index), but can reduce file size and downstream time.

If you tell me how you‚Äôre exporting from DB2 (DB2 EXPORT, db2move, backup restore + export, etc.) and what MariaDB storage engine (likely InnoDB), I‚Äôll give you an exact ‚Äúminimum-impact‚Äù runbook with commands + timings + verification queries.



For 15M rows, reconciliation has to be fast, provable, and not ‚Äúcompare row-by-row manually.‚Äù The usual pattern is:
	1.	Count-level checks (cheap, catches big misses)
	2.	Key/uniqueness checks (catches duplicates & missing keys)
	3.	Aggregate/hash checks by buckets (catches subtle data differences with high confidence)
	4.	Targeted sampling (human-readable spot validation)
	5.	Exception tables (store mismatches so you can fix/re-run only the bad parts)

Below is a concrete reconciliation plan specifically for staging ‚Üí final in MariaDB when you do:

INSERT INTO final ... SELECT ... FROM staging WHERE soft_delete=0;


‚∏ª

A) Baseline record-count reconciliation

1) Total rows

SELECT COUNT(*) AS staging_total FROM staging;
SELECT COUNT(*) AS final_total   FROM final;

2) Active rows vs final rows (this is your core truth)

SELECT COUNT(*) AS staging_active
FROM staging
WHERE soft_delete = 0;

SELECT COUNT(*) AS final_total
FROM final;

‚úÖ Expect: staging_active == final_total

3) Soft-delete rows should be excluded

SELECT COUNT(*) AS staging_deleted
FROM staging
WHERE soft_delete <> 0;

This is mostly sanity.

‚∏ª

B) Key integrity checks (missing & duplicates)

Assume you have a business key (or PK candidate) like id (or composite key).

1) Duplicates in staging among active rows (must be zero before final PK)

SELECT id, COUNT(*) c
FROM staging
WHERE soft_delete = 0
GROUP BY id
HAVING c > 1
LIMIT 20;

If this returns rows, you cannot safely create PK on final without deciding the dedupe rule.

2) Duplicates in final (should be zero if PK exists)

SELECT id, COUNT(*) c
FROM final
GROUP BY id
HAVING c > 1
LIMIT 20;

3) Missing keys in final (anti-join)

This is the most useful ‚Äúdid we lose anything?‚Äù check.

SELECT s.id
FROM staging s
LEFT JOIN final f ON f.id = s.id
WHERE s.soft_delete = 0
  AND f.id IS NULL
LIMIT 100;

‚úÖ Expect: 0 rows

4) Extra keys in final that shouldn‚Äôt be there

SELECT f.id
FROM final f
LEFT JOIN staging s ON s.id = f.id AND s.soft_delete = 0
WHERE s.id IS NULL
LIMIT 100;

‚úÖ Expect: 0 rows

If the final table existed earlier and already had records, this check needs to be constrained to the migrated range/batch.

‚∏ª

C) Content validation beyond keys (fast + strong)

Row counts and keys can match even if a column was transformed wrongly (like your pipe-split case). For 15M rows, do bucketed checksums.

1) Create a deterministic ‚Äúrow fingerprint‚Äù (hash)

Pick the columns that matter most (include the ones transformed).

Example (adjust columns):

-- Staging fingerprint (after applying the same split logic as your insert)
SELECT
  MOD(CRC32(CAST(id AS CHAR)), 1000) AS bucket,
  COUNT(*) AS cnt,
  BIT_XOR(CRC32(CONCAT_WS('#',
      id,
      col1,
      col2,
      CASE WHEN LOCATE('|', acct_src)>0 THEN SUBSTRING_INDEX(acct_src,'|',1) ELSE acct_src END,
      CASE WHEN LOCATE('|', acct_src)>0 THEN SUBSTRING_INDEX(acct_src,'|',-1) ELSE '' END
  ))) AS bucket_xor
FROM staging
WHERE soft_delete = 0
GROUP BY bucket
ORDER BY bucket;

And in final:

SELECT
  MOD(CRC32(CAST(id AS CHAR)), 1000) AS bucket,
  COUNT(*) AS cnt,
  BIT_XOR(CRC32(CONCAT_WS('#',
      id,
      col1,
      col2,
      acct_number,
      COALESCE(acct_indicator,'')
  ))) AS bucket_xor
FROM final
GROUP BY bucket
ORDER BY bucket;

Then compare cnt and bucket_xor bucket-by-bucket.

‚úÖ If both match for all buckets, it‚Äôs a very strong signal the data matches (not mathematically perfect, but excellent in practice for migration QA).

Why buckets?
	‚Ä¢	You avoid one huge hash calculation that can be expensive
	‚Ä¢	Buckets give you pinpoint: ‚Äúbucket 317 is mismatching‚Äù ‚Üí debug just that slice

If BIT_XOR/CRC32 feels too collision-prone for your risk appetite, use SHA2 but it‚Äôs heavier. CRC-based bucket XOR is widely used for speed.

‚∏ª

D) Column-level sanity checks (cheap, catches mapping mistakes)

For each important column:
	‚Ä¢	null rates
	‚Ä¢	min/max
	‚Ä¢	distinct counts (for low-cardinality fields)

Examples:

-- Null rate
SELECT
  SUM(acct_number IS NULL) AS nulls,
  COUNT(*) AS total
FROM final;

-- Range sanity for a numeric column
SELECT MIN(amount), MAX(amount) FROM final;

-- Distinct sanity for a status field
SELECT status, COUNT(*) FROM final GROUP BY status;

Compare these with staging using the same transformations/filters.

‚∏ª

E) Sampling (human validation, not too much)

1) Random sample of keys

SELECT id
FROM staging
WHERE soft_delete = 0
ORDER BY RAND()
LIMIT 200;

Then verify those IDs in final:

SELECT * FROM final WHERE id IN (...);

2) Edge-case sample

Always include:
	‚Ä¢	rows where acct_src contains |
	‚Ä¢	rows where acct_src is empty/null
	‚Ä¢	extreme dates, extreme amounts

Example:

SELECT id, acct_src
FROM staging
WHERE soft_delete = 0
  AND acct_src LIKE '%|%'
LIMIT 200;


‚∏ª

F) Build an ‚Äúexceptions table‚Äù (very practical)

Create a table to store mismatches so you can track issues:

CREATE TABLE recon_exceptions (
  id BIGINT,
  issue VARCHAR(50),
  details VARCHAR(1000),
  created_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

If missing keys found:

INSERT INTO recon_exceptions (id, issue, details)
SELECT s.id, 'MISSING_IN_FINAL', 'present in staging(active) but not in final'
FROM staging s
LEFT JOIN final f ON f.id = s.id
WHERE s.soft_delete = 0
  AND f.id IS NULL;

Now you have a persistent audit trail.

‚∏ª

Recommended reconciliation ‚Äúlevels‚Äù for your case (15M)

Level 1 (must-do)
	‚Ä¢	staging_active == final_count
	‚Ä¢	missing-in-final = 0
	‚Ä¢	duplicates in staging active = 0 (if you plan PK)

Level 2 (strong confidence)
	‚Ä¢	bucketed checksum compare (1000 buckets)

Level 3 (high assurance)
	‚Ä¢	sample + edge-cases
	‚Ä¢	column-level sanity checks

‚∏ª

If you tell me your actual PK/business key (single column or composite) and the top 10 important columns, I can generate the exact SQL reconciliation pack tailored to your schema (including the pipe-split logic you used), so you can run it as a repeatable script after each migration run.


Yes ‚Äî you can and should run the duplicate check on the staging table before creating the PK. That‚Äôs actually the right time to do it.

1) Duplicate check in staging (before PK)

Use:

SELECT id, COUNT(*) AS c
FROM staging
WHERE soft_delete = 0
GROUP BY id
HAVING c > 1
LIMIT 20;

‚úÖ This tells you whether id is truly unique among ‚Äúactive‚Äù rows before you enforce it in final.

‚∏ª

Does id need to be PK or indexed to run this GROUP BY?

Not required, but it affects speed:
	‚Ä¢	If no index on id, MariaDB will do a full scan + sort/temporary table ‚Üí slower on 15M rows, but still doable.
	‚Ä¢	If you add an index on id, the grouping can be significantly faster (depending on engine, memory, and plan).

Best practice for staging

Keep staging with no indexes for fastest load.
Then, only if you need heavy reconciliation queries, create a temporary index after load:

CREATE INDEX idx_stage_id ON staging(id);

If your uniqueness check is important (it is), adding idx_stage_id after load is reasonable.

What about indexing (soft_delete, id)?

If you always filter by soft_delete = 0 first, this is even better:

CREATE INDEX idx_stage_soft_id ON staging(soft_delete, id);

This helps MariaDB quickly isolate active rows and then group by id.

But: creating this index also takes time on 15M rows, so do it only if you‚Äôll run several reconciliation queries.
If you run only one query, you can skip the index and accept a longer runtime.

‚∏ª

In DB2, will this take more time?

Yes ‚Äî and more importantly, it can stress PROD.

Because you said:
	‚Ä¢	DB2 PROD
	‚Ä¢	15M+ rows
	‚Ä¢	no index on soft_delete

Then any query like:

SELECT id, COUNT(*)
FROM prod_table
WHERE soft_delete = 0
GROUP BY id
HAVING COUNT(*) > 1;

will likely cause:
	‚Ä¢	a full table scan
	‚Ä¢	plus a big sort/group operation
	‚Ä¢	with CPU + I/O impact (can affect online workload)

So avoid doing this on PROD DB2 unless:
	‚Ä¢	you can run it on a read replica / HADR standby / restored backup
	‚Ä¢	or it‚Äôs a carefully planned off-peak job

‚úÖ Safer approach: do this check in MariaDB staging after loading.

‚∏ª

Soft delete condition: true or false?

Use the condition that matches what you‚Äôre validating:

If final contains only active rows

Check duplicates on active rows:

WHERE soft_delete = 0

If final contains everything (active + deleted)

Then check duplicates on all rows (or use your business rule).

‚∏ª

What if duplicates exist only because soft-deleted rows share same id?

That‚Äôs common. You might have:
	‚Ä¢	one active row and one soft-deleted row with same id (acceptable)
	‚Ä¢	multiple active rows with same id (not acceptable if id is PK)

So your best validation is:

SELECT id, COUNT(*) c
FROM staging
WHERE soft_delete = 0
GROUP BY id
HAVING c > 1;


‚∏ª

Practical recommendation
	‚Ä¢	‚úÖ Do the duplicate check in MariaDB staging
	‚Ä¢	‚úÖ If it‚Äôs slow, create INDEX(soft_delete, id) on staging after load
	‚Ä¢	‚ùå Don‚Äôt run heavy GROUP BY duplicate checks on DB2 PROD (no index = expensive)

If you share:
	‚Ä¢	is id truly your PK candidate (or composite)?
	‚Ä¢	table size ~15M only or much larger?
I can suggest the exact minimal indexes to create on staging purely for reconciliation (and which ones to avoid).

Yes ‚Äî if staging has duplicates for your intended primary key, then creating the PK on final after the bulk load will fail (because PK requires uniqueness). So you need a dedupe strategy before (or during) the move.

Here are the practical, production-friendly ways to handle it.

‚∏ª

Step 1: Decide what ‚Äúduplicate‚Äù means

Usually duplicates fall into one of these buckets:
	1.	Exact duplicates (all columns same) ‚Üí safe to keep one
	2.	Multiple versions per id (different values) ‚Üí keep the ‚Äúlatest‚Äù or ‚Äúbest‚Äù row
	3.	Soft-delete variations ‚Üí keep active (soft_delete=0) over deleted

You already plan to move only soft_delete=0, so your main problem is:

duplicates among soft_delete=0 rows for the same id

‚∏ª

Option A (Best for big data): Deduplicate in staging into a clean staging table

A1) Create a ‚Äúclean‚Äù staging table that keeps 1 row per id

Pick a rule. Most common rule: keep the row with max(updated_ts) or max(load_ts) or max(sequence).

Example: keep latest by updated_ts:

CREATE TABLE staging_clean AS
SELECT s.*
FROM staging s
JOIN (
  SELECT id, MAX(updated_ts) AS max_ts
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
) x
ON x.id = s.id AND x.max_ts = s.updated_ts
WHERE s.soft_delete = 0;

‚ö†Ô∏è If ties exist (same id + same updated_ts), you may still have duplicates. Then add a tie-breaker (e.g., max(row_id)).

A2) Move staging_clean ‚Üí final

INSERT INTO final (..columns..)
SELECT ..columns..
FROM staging_clean;

A3) Now build PK on final safely

ALTER TABLE final ADD PRIMARY KEY (id);

‚úÖ Pros: deterministic, auditable, easiest to reason about
‚ùå Cons: needs extra storage/time (but safe for 15M)

‚∏ª

Option B: Deduplicate while inserting into final using a GROUP rule

If you have a clear ‚Äúwinner‚Äù rule, you can do it in one step:

Example: keep row with MAX(updated_ts) per id:

INSERT INTO final (..)
SELECT s..
FROM staging s
JOIN (
  SELECT id, MAX(updated_ts) AS max_ts
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
) x
ON s.id = x.id AND s.updated_ts = x.max_ts
WHERE s.soft_delete = 0;

‚úÖ No extra table
‚ùå Still heavy join; tie handling needed

‚∏ª

Option C: Use final with PRIMARY KEY from the beginning + ‚Äúupsert‚Äù

This is often the simplest if your rule is ‚Äúlatest wins‚Äù and you can express it.

C1) Create final with PK upfront

CREATE TABLE final (
  id BIGINT NOT NULL,
  ...,
  updated_ts DATETIME,
  PRIMARY KEY (id)
) ENGINE=InnoDB;

C2) Load using INSERT ... ON DUPLICATE KEY UPDATE

INSERT INTO final (id, ..., updated_ts)
SELECT id, ..., updated_ts
FROM staging
WHERE soft_delete = 0
ON DUPLICATE KEY UPDATE
  -- choose columns from the row you want to keep
  updated_ts = GREATEST(final.updated_ts, VALUES(updated_ts)),
  col1 = IF(VALUES(updated_ts) >= final.updated_ts, VALUES(col1), final.col1),
  col2 = IF(VALUES(updated_ts) >= final.updated_ts, VALUES(col2), final.col2);

‚úÖ Pros: guarantees uniqueness as you load
‚ùå Cons: can be slower than pure bulk insert (because it‚Äôs doing per-row PK checks + updates)

For 15M rows, this is OK if duplicates are rare; if duplicates are many, it can get slow.

‚∏ª

Option D (If duplicates are only exact duplicates): DISTINCT / hash

If duplicates are literally identical rows:

INSERT INTO final (...)
SELECT DISTINCT ...
FROM staging
WHERE soft_delete = 0;

‚úÖ Very simple
‚ùå Can be expensive; only works for exact duplicates (not conflicting values)

‚∏ª

What I recommend for your case (15M, bulk load, want performance)

Use Option A (clean staging table) unless storage is extremely tight.

Reason:
	‚Ä¢	You can prove which record you kept for each id
	‚Ä¢	You can keep a duplicate audit table
	‚Ä¢	PK creation will never surprise-fail at the end

‚∏ª

Add an ‚Äúaudit‚Äù for duplicates (very important)

Before dedupe, store duplicates for investigation:

CREATE TABLE staging_dupes AS
SELECT s.*
FROM staging s
JOIN (
  SELECT id
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
  HAVING COUNT(*) > 1
) d ON d.id = s.id
WHERE s.soft_delete = 0;

Now you can answer: how many duplicates, which ids, why?

‚∏ª

Key point about PK creation timing
	‚Ä¢	If you bulk-load into final without PK, then:
	‚Ä¢	PK add will fail if duplicates exist
	‚Ä¢	you‚Äôll have to clean final and retry (wasteful)

So if duplicates are possible:
	‚Ä¢	either dedupe before insert, or
	‚Ä¢	create PK first and use upsert logic

‚∏ª

Tell me these 3 fields and I‚Äôll give you the exact SQL
	1.	What is the dedupe rule you want?
	‚Ä¢	latest by updated_ts?
	‚Ä¢	highest sequence?
	‚Ä¢	first row wins?
	2.	Do you have a reliable timestamp / sequence column in DB2?
	3.	Is id single PK or composite (id + tenant_id etc.)?

With that, I‚Äôll produce a ready-to-run migration + dedupe + reconciliation script.


If you don‚Äôt have any ‚Äúlatest‚Äù indicator (no timestamp, sequence, version), then duplicates are fundamentally ambiguous ‚Äî you can‚Äôt correctly choose the ‚Äúright‚Äù row by logic. So the ‚Äúright solution‚Äù is:
	1.	Prevent the migration from silently picking a random row, and
	2.	Make duplicates visible + reviewable, while still allowing the bulk of good data to move.

‚úÖ Yes: creating final with PK and inserting with ‚Äúignore duplicates‚Äù is technically workable ‚Äî but it‚Äôs risky

Because which row gets kept is arbitrary (depends on load order / file order / engine behavior). That‚Äôs fine only if:
	‚Ä¢	duplicates are exact duplicates (same values), or
	‚Ä¢	business says ‚Äúany one is acceptable‚Äù.

If duplicates have different values, using IGNORE is essentially data-loss without trace unless you audit.

So if you do it, do it with an audit plan.

‚∏ª

The safest practical pattern (recommended)

Step 0: Detect how bad duplicates are

SELECT COUNT(*) AS dup_ids
FROM (
  SELECT id
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
  HAVING COUNT(*) > 1
) x;

Also measure volume:

SELECT SUM(c) AS dup_rows_total
FROM (
  SELECT COUNT(*) c
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
  HAVING COUNT(*) > 1
) x;


‚∏ª

Option 1 (Recommended): Load clean rows + quarantine duplicates

This avoids random selection and keeps the migration honest.

1A) Move only IDs that are unique

INSERT INTO final (id, col1, col2, ...)
SELECT s.id, s.col1, s.col2, ...
FROM staging s
JOIN (
  SELECT id
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
  HAVING COUNT(*) = 1
) u ON u.id = s.id
WHERE s.soft_delete = 0;

1B) Put duplicates into a separate table for manual/business resolution

CREATE TABLE staging_dupes AS
SELECT s.*
FROM staging s
JOIN (
  SELECT id
  FROM staging
  WHERE soft_delete = 0
  GROUP BY id
  HAVING COUNT(*) > 1
) d ON d.id = s.id
WHERE s.soft_delete = 0;

‚úÖ Pros:
	‚Ä¢	final data = guaranteed unique and clean
	‚Ä¢	duplicates are not lost
	‚Ä¢	you can resolve duplicates later and insert them with business approval

‚ùå Cons:
	‚Ä¢	requires follow-up process for duplicate IDs

This is the best ‚Äúno latest flag‚Äù answer.

‚∏ª

Option 2: If duplicates are expected to be identical ‚Üí dedupe by exact row content

If duplicates are genuinely identical rows, you can keep one safely.

Example approach:
	‚Ä¢	Create a ‚Äúrow signature‚Äù hash and keep one row per (id, signature)

But in practice with many columns this is heavy and still needs care.

‚∏ª

Option 3: Create final with PK and use INSERT IGNORE (allowed, but only with audit)

If business accepts ‚Äúfirst row wins‚Äù, then do:

3A) Create final with PK upfront

CREATE TABLE final (
  id BIGINT NOT NULL,
  ...,
  PRIMARY KEY (id)
) ENGINE=InnoDB;

3B) Insert, ignoring duplicates

INSERT IGNORE INTO final (id, col1, col2, ...)
SELECT id, col1, col2, ...
FROM staging
WHERE soft_delete = 0;

‚úÖ Pros:
	‚Ä¢	simplest
	‚Ä¢	avoids migration failure
	‚Ä¢	fast enough in many cases

‚ö†Ô∏è Cons:
	‚Ä¢	keeps whichever duplicate appears first
	‚Ä¢	silently drops the rest unless you capture them

3C) Audit what got dropped (must-do if you use IGNORE)

After insert:

CREATE TABLE final_dup_rejects AS
SELECT s.*
FROM staging s
LEFT JOIN final f ON f.id = s.id
WHERE s.soft_delete = 0
  AND f.id IS NULL;

This table will contain rows that did not make it into final (mostly dupes, but also any failed conversions if you have them).

‚∏ª

What is the ‚Äúright solution‚Äù?

Given your constraints (no latest field), the most correct and defensible approach is:

‚úÖ Quarantine duplicates, load only unique IDs
	‚Ä¢	It avoids random choice
	‚Ä¢	It preserves all raw conflicting rows
	‚Ä¢	It gives the business/data owner a chance to decide resolution rules

Only use INSERT IGNORE if you have explicit agreement that:
	‚Ä¢	duplicates are identical, or
	‚Ä¢	any one row is acceptable, and you keep an audit table.

‚∏ª

Small but important note about ‚Äúsoft_delete‚Äù

Always do the duplicate detection on the same population you load:

WHERE soft_delete = 0

Because duplicates might only exist due to deleted history rows, which may be acceptable.

‚∏ª

If you want, I can propose a resolution rule that doesn‚Äôt need ‚Äúlatest‚Äù

Sometimes teams use a deterministic tie-breaker even without timestamps, for example:
	‚Ä¢	prefer row with more non-null fields
	‚Ä¢	prefer row with higher ‚Äúquality score‚Äù
	‚Ä¢	prefer row with a valid pattern (e.g., AccountIndicator not null)
	‚Ä¢	prefer lexicographically max/min of a stable field

It‚Äôs not ‚Äúlatest‚Äù, but it can be a business-quality rule.

If you share 3‚Äì5 key columns (the ones that differ across duplicates), I can suggest a safe deterministic scoring rule and the SQL to implement it.


